{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa967a0",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebbf9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import random_split, ConcatDataset, DataLoader, TensorDataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch import Tensor\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device is {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e194e",
   "metadata": {},
   "source": [
    "### models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636014a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_size, latent_size ):\n",
    "        super().__init__()\n",
    "        self.__latent_size = latent_size\n",
    "        self.fc1 = nn.Linear(x_size + y_size, 128)\n",
    "        self.fc2 = nn.Linear(128, latent_size)\n",
    "        self.fc3 = nn.Linear(128, latent_size)\n",
    "        self.fc4 = nn.Linear(latent_size + y_size, 256)\n",
    "        self.fc5 = nn.Linear(256, x_size)\n",
    "\n",
    "\n",
    "    def encoder(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc2(x)\n",
    "        log_var = self.fc3(x)\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "\n",
    "    def sample(self, y):\n",
    "        z = torch.randn(y.size(0), self.__latent_size, device=y.device)\n",
    "        return self.decoder(torch.cat((z, y), dim=1)), z\n",
    "    \n",
    "\n",
    "    def decoder(self, z):\n",
    "        z = F.relu(self.fc4(z))\n",
    "        x_hat = self.fc5(z)\n",
    "        return x_hat\n",
    "    \n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_hat = self.decoder(torch.cat((z, y), dim=1))\n",
    "        return x_hat, mu, log_var\n",
    "    \n",
    "\n",
    "    def train_loss(self, x, y):\n",
    "        x_hat, mu, log_var = self.forward(x, y)\n",
    "        MSE = F.mse_loss(x_hat, x, reduction='mean')\n",
    "        KLD = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        return MSE + KLD\n",
    "    \n",
    "\n",
    "class MultiView(nn.Module):\n",
    "\n",
    "    def __init__(self, multiview_mask):\n",
    "        super().__init__()\n",
    "        self.multiview_mask = multiview_mask\n",
    "        view_count, input_size = multiview_mask.shape\n",
    "        self.bn = nn.BatchNorm1d(input_size)\n",
    "        # Performance optimization: parallel compute\n",
    "        # https://stackoverflow.com/a/58389075/318557\n",
    "        self.mlp_parallel_cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_size * view_count, 64*view_count, kernel_size = 1, stride = 1, padding = 0, groups = view_count),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64*view_count, 128*view_count, kernel_size = 1, stride = 1, padding = 0, groups = view_count),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128*view_count, input_size * view_count, kernel_size = 1, stride = 1, padding = 0, groups = view_count),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.mlp_classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.get_score(x)\n",
    "        x = mask * x\n",
    "        y_hat = self.mlp_classifier(x)\n",
    "        return y_hat, mask\n",
    "    \n",
    "\n",
    "    def get_score(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = x.unsqueeze(1).repeat(1, self.multiview_mask.shape[0], 1)\n",
    "        x = self.multiview_mask * x\n",
    "        x = torch.flatten(x, start_dim=1).unsqueeze(2)\n",
    "        x = self.mlp_parallel_cnn(x)\n",
    "        x = x.view(x.shape[0], self.multiview_mask.shape[0], -1)\n",
    "        x = self.multiview_mask * x\n",
    "        x = x.sum(dim=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def train_loss(self, x, y):\n",
    "        y_hat, mask = self.forward(x)\n",
    "        return F.cross_entropy(y_hat, y.view(-1).long()) + 2.2e-3 * mask.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195864ea",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datas import load_data\n",
    "LOAD_PATH = \"./data/\"\n",
    "# samples size: (47, 392)\n",
    "# labels size: (47,)\n",
    "samples, labels = load_data(LOAD_PATH)\n",
    "std, mean = torch.std_mean(samples, dim=0)\n",
    "samples = ((samples - mean) / std).to(DEVICE)\n",
    "labels = labels.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655fd2c",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b909b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(samples: Tensor, labels: Tensor):\n",
    "    data_p = samples[labels.view(-1) == 1]\n",
    "    data_n = samples[labels.view(-1) == 0]\n",
    "    split_ratio = (0.6, 0.4)\n",
    "    train_p, test_p = random_split(TensorDataset(data_p, torch.ones(len(data_p), 1, device=DEVICE)), split_ratio)\n",
    "    train_n, test_n = random_split(TensorDataset(data_n, torch.zeros(len(data_n), 1, device=DEVICE)), split_ratio)\n",
    "    train_set = ConcatDataset([train_p, train_n])\n",
    "    test_set = ConcatDataset([test_p, test_n])\n",
    "    train_loader = DataLoader(train_set, batch_size=47, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=47, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def print_classification_result(cm: torch.Tensor) -> None:\n",
    "    acc = (cm[:, 0, 0] + cm[:, 1, 1]) / cm.sum(dim=(1, 2))\n",
    "    precision = cm[:, 1, 1] / (cm[:, 1, 1] + cm[:, 0, 1])\n",
    "    recall = cm[:, 1, 1] / (cm[:, 1, 1] + cm[:, 1, 0])\n",
    "    fpr = cm[:, 0, 1] / (cm[:, 0, 1] + cm[:, 0, 0])\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    n = len(cm)\n",
    "    acc_std, acc_mean = torch.std_mean(acc)\n",
    "    precision_std, precision_mean = torch.std_mean(precision)\n",
    "    recall_std, recall_mean = torch.std_mean(recall)\n",
    "    fpr_std, fpr_mean = torch.std_mean(fpr)\n",
    "    f1_std, f1_mean = torch.std_mean(f1)\n",
    "    sqrt_n = torch.sqrt(torch.tensor(n, dtype=float))\n",
    "    print(f\"acc: {acc_mean * 100:.2f}%±{1.96 * acc_std * 100 / sqrt_n:.2f}%.\")\n",
    "    print(f\"precision: {precision_mean * 100:.2f}%±{1.96 * precision_std * 100 / sqrt_n:.2f}%.\")\n",
    "    print(f\"recall: {recall_mean * 100:.2f}%±{1.96 * recall_std * 100 / sqrt_n:.2f}%.\")\n",
    "    print(f\"fpr: {fpr_mean * 100:.2f}%±{1.96 * fpr_std * 100 / sqrt_n:.2f}%.\")\n",
    "    print(f\"f1: {f1_mean * 100:.2f}%±{1.96 * f1_std * 100 / sqrt_n:.2f}%.\")\n",
    "\n",
    "\n",
    "def data_augmentation(train_loader, input_size, da_count=256):\n",
    "    epochs = 500\n",
    "    learning_rate = 4.7e-3\n",
    "    latent_size =  16\n",
    "    cvae = CVAE(input_size, 1, latent_size).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "    for _ in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = cvae.train_loss(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        y_ = torch.bernoulli(torch.ones(da_count, 1, device=DEVICE) * 0.5)\n",
    "        x_, z_ = cvae.sample(y_)\n",
    "    return x_, y_\n",
    "\n",
    "\n",
    "def feature_cluster(x):\n",
    "    x = x.detach().cpu().numpy()\n",
    "    kmeans = KMeans(n_clusters=8)\n",
    "    k_mean_mask = F.one_hot(torch.tensor(kmeans.fit_predict(x.T),dtype=torch.long))\n",
    "    return k_mean_mask.T\n",
    "\n",
    "\n",
    "def check_features(samples, labels, selector, use_da=False, repeat=10):\n",
    "    cm = []\n",
    "    y_softmax = []\n",
    "    for _ in range(repeat):\n",
    "        train_loader, test_loader = split_dataset(samples, labels)\n",
    "        train_samples, train_labels = data_augmentation(train_loader, samples.shape[1], da_count=256) if use_da else next(iter(train_loader))\n",
    "        train_samples = train_samples[:, selector] if selector != None else train_samples\n",
    "        train_loader = DataLoader(TensorDataset(train_samples, train_labels), batch_size=32, shuffle=True)\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Linear(train_samples.shape[1], 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        ).to(device=DEVICE)\n",
    "        optimizer = Adam(classifier.parameters(), lr=1.0e-3, weight_decay=1.0e-5)\n",
    "        for epoch in range(16):\n",
    "            for x, y in train_loader:\n",
    "                y_hat = classifier(x)\n",
    "                loss = F.cross_entropy(y_hat, y.view(-1).long())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            x, y = next(iter(test_loader))\n",
    "            x = x[:, selector] if selector != None else x\n",
    "            y = y.view(-1)\n",
    "            y_hat = classifier(x)\n",
    "            y_softmax.append(y_hat)\n",
    "            cm.append(torch.tensor(confusion_matrix(y.cpu().numpy(), y_hat.argmax(dim=1).cpu().numpy())))\n",
    "    cm = torch.stack(cm)\n",
    "    y_softmax = torch.stack(y_softmax)\n",
    "    print_classification_result(cm)\n",
    "    print()\n",
    "    return cm, y_softmax\n",
    "\n",
    "\n",
    "def chekc_stablility(marker: torch.Tensor):\n",
    "    dh = []\n",
    "    n = len(marker)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dh.append((marker[i] != marker[j]).sum().item())\n",
    "    dh = torch.tensor(dh, dtype=torch.float)\n",
    "    dh_std, dh_mean = torch.std_mean(dh)\n",
    "    print(f\"dh: {dh_mean:.2f}±{1.96 * dh_std / np.sqrt(len(marker)):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67768e",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 300\n",
    "multiview_mask = feature_cluster(samples).to(DEVICE)\n",
    "scores = []\n",
    "for _ in range(repeat):\n",
    "    train_loader, test_loader = split_dataset(samples, labels)\n",
    "    train_samples, train_labels = data_augmentation(train_loader, samples.shape[1], da_count=256)\n",
    "    train_loader = DataLoader(TensorDataset(train_samples, train_labels), batch_size=32, shuffle=True)\n",
    "    mvfs = MultiView(multiview_mask).to(DEVICE)\n",
    "    params = [{\n",
    "        \"params\": mvfs.mlp_parallel_cnn.parameters(),\n",
    "        \"weight_decay\": 1.0e-6\n",
    "    }, {\n",
    "        \"params\": mvfs.mlp_classifier.parameters(),\n",
    "        \"weight_decay\": 1.0e-4\n",
    "    }, {\n",
    "        \"params\": mvfs.bn.parameters(),\n",
    "        \"weight_decay\": 1.0e-6\n",
    "    }]\n",
    "    optimizer = torch.optim.Adam(params, lr=1.0e-3)\n",
    "    mvfs.train(True)\n",
    "    epochs = 500\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            loss = mvfs.train_loss(x, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        mvfs.train(False)\n",
    "        score = mvfs.get_score(train_samples).mean(dim=0)\n",
    "        scores.append(score)\n",
    "threshold = 0.6\n",
    "scores = torch.stack(scores)\n",
    "chekc_stablility(scores > threshold)\n",
    "score_std, score_mean = torch.std_mean(scores, dim=0)\n",
    "pick = score_mean > threshold\n",
    "check_features(samples, labels, pick, use_da=True, repeat=repeat)\n",
    "torch.save(scores, \"save.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
